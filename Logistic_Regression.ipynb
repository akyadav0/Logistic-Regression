{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "rtorw3vehtMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**"
      ],
      "metadata": {
        "id": "LO_EgVu2hvq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "- Logistic Regression is used for classification problems (output is categorical), while Linear Regression is for regression (output is continuous)."
      ],
      "metadata": {
        "id": "h-XuqMtjh1E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n",
        "- p(x) = 1 / (1 + e^-(β0 + β1 * x))\n"
      ],
      "metadata": {
        "id": "awwUsULckxeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "- The sigmoid function maps any real-valued number into the range (0, 1), making it ideal for representing probabilities in binary classification. It transforms the linear combination of inputs into a probability score"
      ],
      "metadata": {
        "id": "WOO2rm9BqDpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?\n",
        "- The cost function is the cross-entropy loss (log loss), which measures the difference between predicted probabilities and actual class labels. It is derived from the log-likelihood function and optimized using maximum likelihood estimation"
      ],
      "metadata": {
        "id": "7Lqohh4VqO78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "- Regularization is a technique to prevent overfitting by adding a penalty for large coefficients to the cost function. It helps improve generalization to new data by discouraging overly complex models"
      ],
      "metadata": {
        "id": "DRuw9mreqcTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "- Lasso (L1): Adds absolute value of coefficients as penalty, promoting sparsity (feature selection).\n",
        "\n",
        "- Ridge (L2): Adds squared value of coefficients as penalty, shrinking coefficients but not setting them to zero."
      ],
      "metadata": {
        "id": "VaPm6mZesbZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "- Elastic Net is preferred when there are multiple correlated features or when both feature selection and coefficient shrinkage are desired. It combines the strengths of Lasso and Ridge"
      ],
      "metadata": {
        "id": "lcfA9qr-umop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "- The regularization parameter (often denoted as λ or its inverse C in scikit-learn) controls the strength of the penalty. A higher value increases regularization, shrinking coefficients more and reducing overfitting, but may underfit if set too high"
      ],
      "metadata": {
        "id": "otcfF7pMutkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "- The outcome is binary or categorical.\n",
        "\n",
        "- Observations are independent.\n",
        "\n",
        "- There is little or no multicollinearity among predictors."
      ],
      "metadata": {
        "id": "LOf-HC92uvN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "- Alternatives include Decision Trees, Random Forests, Support Vector Machines, Naive Bayes, and Neural Networks."
      ],
      "metadata": {
        "id": "NrqnPFQnuzy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n",
        "- Common metrics include Accuracy, Precision, Recall, F1 Score, ROC-AUC, and Log Loss."
      ],
      "metadata": {
        "id": "M7hyQbiGu4lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?\n",
        "- Class imbalance can bias the model towards the majority class, reducing performance on the minority class. This can lead to misleading accuracy and poor recall for the minority class."
      ],
      "metadata": {
        "id": "5xxE5Z71u6Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "- It involves optimizing model parameters such as regularization strength (C or λ), penalty type (L1, L2, Elastic Net), and solver choice to improve model performance, often using grid search or randomized search"
      ],
      "metadata": {
        "id": "NKz_fXKEu9KV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "- liblinear: Good for small datasets, supports L1 and L2 penalties.\n",
        "\n",
        "- lbfgs: Efficient for medium/large datasets, supports L2 penalty."
      ],
      "metadata": {
        "id": "QAIpUfvwvcfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "- Through strategies like One-vs-Rest (OvR) or using the multinomial (softmax) loss function with suitable solvers (e.g., lbfgs, saga)"
      ],
      "metadata": {
        "id": "SNJG8P8wvq75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "- Advantages: Simple, interpretable, efficient, works well for linearly separable data.\n",
        "- Disadvantages: Limited to linear decision boundaries, sensitive to outliers, struggles with non-linear relationships."
      ],
      "metadata": {
        "id": "GTySp34MvwI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?\n",
        "- Spam detection, credit scoring, disease diagnosis, customer churn prediction, and marketing response prediction."
      ],
      "metadata": {
        "id": "wDZt4jpXv__K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "- Logistic Regression is for binary classification; Softmax Regression (multinomial logistic regression) generalizes it to multiclass problems by predicting probabilities across multiple classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "P2JcKT7bwDxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "- OvR trains one classifier per class versus all others, while Softmax (multinomial) trains a single model for all classes. Softmax is generally preferred for mutually exclusive classes and when using solvers that support it (e.g., lbfgs, saga)"
      ],
      "metadata": {
        "id": "YnBccURrwG-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "- Coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor. Exponentiating a coefficient gives the odds ratio for that predictor."
      ],
      "metadata": {
        "id": "H929pSd6wNTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**"
      ],
      "metadata": {
        "id": "kb95EifLwRyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWJLgB9RwZbx",
        "outputId": "c65cfdf7-e1eb-4951-f79a-e52d63aafe51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "QM8stbjdziqY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Hc7nLxYjz051"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"ElasticNet Regularized Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHV1KZgP1E1b",
        "outputId": "681a952e-8af0-4664-c887-31f5219639b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet Regularized Accuracy: 0.9035087719298246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ],
      "metadata": {
        "id": "THnDltT61s9Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlKSaKQW2CD4",
        "outputId": "454d7892-c3b8-4dcb-ae47-b952997fceb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9626373626373628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Stratified K-Fold Average Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K88-kG02Lsi",
        "outputId": "9db3c68b-d92c-4738-bed3-3a039d230aab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified K-Fold Average Accuracy: 0.9525694767893185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample: Replace 'your_file.csv' and adjust column names\n",
        "# Make sure 'your_file.csv' exists and 'target_column' is the correct column name\n",
        "try:\n",
        "    df = pd.read_csv('your_file.csv')\n",
        "    X = df.drop('target_column', axis=1)\n",
        "    y = df['target_column']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"CSV Data Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'your_file.csv' was not found.\")\n",
        "except KeyError:\n",
        "    print(\"Error: The 'target_column' was not found in the DataFrame. Please check the column name.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK2EBB_F5lPO",
        "outputId": "e5452d53-53ad-45cb-a759-f2b94660fa52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file 'your_file.csv' was not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_grid, cv=5, scoring='accuracy')\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, random_search.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge3WQrrkcvjX",
        "outputId": "8c66d97d-2f1b-4cc9-e22f-5bbc8fef6116"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression())\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gq_4daOc7NE",
        "outputId": "15c11b18-6f30-43e1-ca44-784977534753"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "c_Qg3skidDnH",
        "outputId": "467ab555-5fd4-4746-edbe-42d776ad38ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fcde897f250>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6F6FMaRdDkx",
        "outputId": "7f755c1e-9e9a-4b0f-9df5-28d076c56103"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTQi2vmrdDiY",
        "outputId": "894ca854-7699-4864-9655-7456324facb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "9-c7-BY-dDfx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFMz8psZdDdL",
        "outputId": "0be54dc0-985e-4610-b2de-035fa6d9e60f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model for multiclass classification\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Topl7KAydDao",
        "outputId": "2a1de937-77ab-4b2e-d2a7-5c4c11fcabc0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tcs0YJadDYO",
        "outputId": "a0eb66d2-2c6d-4b63-f489-23da43c893b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.Series(model.coef_[0], index=data.feature_names)\n",
        "print(\"Important Features:\")\n",
        "print(feature_importance.sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jilYQB8ydDVo",
        "outputId": "ffc83eac-d60d-4099-e5d7-4cbe5af816be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features:\n",
            "sepal width (cm)     0.868922\n",
            "sepal length (cm)   -0.405385\n",
            "petal width (cm)    -0.956801\n",
            "petal length (cm)   -2.277875\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Cohen’s Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GcoTnHSdDSy",
        "outputId": "7b2a20a4-9fd1-47f9-bcf7-ac602244748b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen’s Kappa Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lf04EssodDQN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Solver: {solver} - Accuracy: {accuracy_score(y_test, model.predict(X_test))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WGJr17DdDGR",
        "outputId": "a9291671-7da0-422a-c322-ec3df6e694ee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear - Accuracy: 0.9777777777777777\n",
            "Solver: saga - Accuracy: 1.0\n",
            "Solver: lbfgs - Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWG-F3bqfJHS",
        "outputId": "ee0e5ed3-61eb-4f21-b0b8-8f802ece04dc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "R7UVErBxfIxd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "for c in C_values:\n",
        "    model = LogisticRegression(C=c, max_iter=1000)\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "    print(f\"C={c} | Accuracy: {np.mean(scores):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAG9ixeifItN",
        "outputId": "7a6814ff-107a-4039-f760-26741d432767"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.01 | Accuracy: 0.8600\n",
            "C=0.1 | Accuracy: 0.9467\n",
            "C=1 | Accuracy: 0.9733\n",
            "C=10 | Accuracy: 0.9733\n",
            "C=100 | Accuracy: 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import joblib\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy0dC67TfIoq",
        "outputId": "0dcbb584-bc55-45b5-b3bb-653ef8741cfe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['logistic_regression_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}